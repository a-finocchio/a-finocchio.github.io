<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Profolio / 3D Visual SLAM</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

	 	<!-- Wrapper -->
		<div id="wrapper">

			<div id="wrapper" class="header">
				<p> Huixiang Li</p>
			</div>

			<!-- Nav -->
			<nav id="nav">
				<ul class="links">
					<li class="active"><a href="portfolio.html">PORTFOLIO </a></li>
					<!-- <li><a href="resorces.html">RESOURCES </a></li> -->
					<li><a href="AboutMe.html">ABOUT ME</a></li>
				</ul>
				<ul class="icons">
					<li><a href="https://github.com/a-finocchio" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
				</ul>
			</nav>

			<!-- Main -->
			<div id="main">
				<p> Portfolio / 3D visual SLAM and path planning with drone</p>

				<section class="post">

					<header class="major">					
						<h2>3D Visual SLAM and Path Planning with drone</h2>								
						<span class="date">March - May, 2018</span>
					</header>

					<div class="image main"><img src="images/Protfolio/3D_visual_slam/poster.jpg" alt="" />
					<span class="date">Figure &nbsp 1 : &nbsp Porject poster</span>
					</div>


					<h3>Introduction</h3>
						<p>
						This project focuses on a fusion of monocular vision and IMU to robustly track the position of an AR-drone using LSD-SLAM (Large-Scale Direct Monocular SLAM) algorithm. The system consists of a low-cost commercial drone and a remote control unit to computationally afford the SLAM algorithms using a distributed node system based on ROS (Robot Operating System). Upon finishing this project, it is expected that we are able to reconstruct the 3D environment around the AR-drone and localize the itself. 
						Simultaneous Localization and Mapping (SLAM) for Unmanned Aerial Vehicles (UAV) in the context of rescue and/or recognition navigation tasks in indoor environments has been a hot topic for several years.
						</br><a href="https://vision.in.tum.de/research/vslam/lsdslam"><b>LSD-SLAM</b></a> is a novel, direct monocular SLAM technique developed by TUM: Instead of using keypoints, it directly operates on image intensities both for tracking and mapping. The camera is tracked using direct image alignment, while geometry is estimated in the form of semi-dense depth maps, obtained by filtering over many pixel-wise stereo comparisons. 
						</p>
					</br> 


					<h3>Proposed Goals</h3>
						<p>
						This project focuses on a fusion of monocular vision and IMU to robustly track the position of an AR-drone using LSD-SLAM (Large-Scale Direct Monocular SLAM) [1, 2] algorithm. The system consists of a low-cost commercial drone and a remote control unit to computationally afford the SLAM algorithms using a distributed node system based on ROS (Robot Operating System). Upon finishing this project, it is expected that we are able to reconstruct 
						the 3D environment around AR-drone and localize itself. In addition, using visual cues, the drone will be able to hold a given position despite the random disturbances that could be applied to the drone as well as navigate to a given position or follow a certain path autonomously and safely within the map built with LSD-SLAM. The built map will be displayed on the host computer. The drone then will be able to follow the generated path according to the octomap and avoid obstacles accordingly. 
						</p>
					</br> 


					<h3>Software</h3>

						<p><b>Existing software employed by this project include the following:</b>
						</br>1. ardrone_autonomy: ROS driver for Parrot AR-Drone 1.0 & 2.0 quadrotor.&nbsp <a href="http://wiki.ros.org/ardrone_autonomy">LINK</a>
						</br>2. LSD_SLAM: It is a novel approach to real-time monocular SLAM. It is fully direct (i.e. does not use keypoints / features) and creates large-scale, semi-dense maps in real-time on a laptop. &nbsp <a href=" https://vision.in.tum.de/research/vslam/lsdslam"> LINK</a>
						</br> 3. tum_ardrone: It consists of three components: a monocular SLAM system, an extended Kalman filter for data fusion and state estimation and a PID controller to generate steering commands. &nbsp <a href="http://wiki.ros.org/tum_ardrone">LINK</a>
						</br> 4. image_proc: This node rectify the raw image captured by the front camera of AR-drone. By our experiment, this step can significantly reduce the noise in the point cloud map generated by LSD-SLAM. 
						</p>                                       

						<p><b>New software that we designed and coded for this project include the following:</b>
						</br>1. cvg_sim_gazebo: Gazebo simulation for Hydro Lab. 
						</br>2. ardrone_joystick: use Logitech joystick to publish cmd_vel to control the motion of AR-Drone.
						</br> 3. point_cloud_io: publish point cloud topic generated from LSD-SLAM.
						</br>4. ardrone_moveit: subscribe point cloud topic and convert point cloud data into octomap for visualization and do path planning,then publish trajectory with fake joint states. 
						</br>5. ardrone_planning: subscribe trajectory with fake joints states from ardrone_moveit and convert them to drone’s pose states, then publish cmd_vel to the AR-Drone. 
						</p>
						
						<div class="image main"><p><b>Program flow:</b></p><img src="images/Protfolio/3D_visual_slam/software.jpg" alt="" />
							<span class="date">Figure &nbsp 2 : &nbsp program architechture</span>
						</div>
					
						<div class="image main"><p><b>Architechture in ROS:</b></p><img src="images/Protfolio/3D_visual_slam/rqt_graph.jpg" alt="" />
							<span class="date">Figure &nbsp 3 : &nbsp rqt_graph</span>
						</div>
					</br> 


					<h3>Hardware and Infrastructure</h3>

						<p><b>Existing hardware employed by this project include the following:</b>
						</br>1. Laptops with ROS Kinetic and Ubuntu 16.04 installed. 
						</br>2. AR Drone: a low cost quadrotor developed by Parrot.
						</br> 3. Logitech Joystick: a low cost multi purpose joystick for teleporated control of the drone.
						</p>
						<div class="image main"><img src="images/Protfolio/3D_visual_slam/hardware.jpg" alt="" />
							<span class="date">Figure &nbsp 4 : &nbsp AR-drone and joystick</span>
						</div>
					</br>
					
					
					<h3>Sample Data Products </h3>	

						<div class="image main"><img src="images/Protfolio/3D_visual_slam/real_envir.jpg" alt="" />
							<span class="date">Figure &nbsp 5 : &nbsp the real environment</span>
						</div>

						<div class="image main"><img src="images/Protfolio/3D_visual_slam/pointcloud.jpg" alt="" />
							<span class="date">Figure &nbsp 6 : &nbsp point cloud map generated by LSD-SLAM</span>
						</div>

						<div class="image main"><img src="images/Protfolio/3D_visual_slam/octomap.jpg" alt="" />
							<span class="date">Figure &nbsp 7 : &nbsp octomap from point cloud for path planning</span>
						</div>
					</br>


					<h3>Result and Demo video</h3>

						<div class="image main"><img src="images/Protfolio/3D_visual_slam/result.png" alt="" />
							<span class="date">Figure &nbsp 8 : &nbsp generating point cloud(right) from monocular camera(left)</span>
						</div>
						<p>A simple environment and the result of map is shown above. The AR-Drone is able to map the stairs and obstacles as well as localize itself. For more detailed explanations, you can refer to the attached video in the next section. 	
						</p>

						<p><b>Demo video:</b></p>
						<div class="video_responsive">
								<iframe width="560" height="315" src="https://www.youtube.com/embed/ysaGH9HKCDw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						</div>
					</br></br>


					<h3>Lessons learned</h3>

						<p>• Upgrade Indigo package to Kinetic package and migrate rosbuild file to catkin.
						</br> • Convert point cloud data to octomap and plug it into MoveIt! for path planning. 
						</br>• For 3D path navigation, using MoveIt! would be better than Move_base. Typically, MoveIt! relies on pre-defined action files and action controller file (.yaml file) for translating the multi DOF trajectories produced by MoveIt!.
						</br> • MoveIt! does not have a good support for mobile robot. Therefore, we should treat the quadrotor as a multi DOF joint and use fake joint_states when connecting MoveIt! and the AR Drone. 
						</br>• A server on the quadrotor need to service the move_group client in order to receive control commands output by the move_group node. 
						</p>
					</br>


					<h3>Suggestions</h3>

						<p>	• Add a filter to reduce noise of point cloud data generated by LSD-SLAM in real time. 
						</br>• Update the octomap periodically in Rviz.
						</br> • Use both PTAM and LSD-SLAM to improve the precision of pose estimation.	
						</p>
					</br>	


					<h3>References</h3>

						<p>	[1] J. Engel, T. Schops, and D. Cremers, “Lsd-slam: Large-scale direct monocular slam,” in Computer Vision–ECCV 2014, pp. 834–849, Springer, 2014.  
						</br>[2] J. Engel, J. Sturm, and D. Cremers, “Camera-based navigation of a low-cost quadrocopter,” in IROS, 2012. 
						</br> [3]“ardrone_autonomy.” ardrone_autonomy-ardrone_autonomy Indigo-Devel Documentation, ardrone-autonomy.readthedocs.io/en/latest/. 
						</p>
					</br>	


					<h3>Code &nbsp -> &nbsp <a href="https://github.com/a-finocchio/3D_visual_SLAM_Using_AR-drone" class="icon brands fa-github"><span class="label">GitHub</span></a></h3>
					</br>


					<h3>Credit:</h3>

						<p>Mentor: &nbsp Prof. Louis Whitcomb 
						</br>Teammates: &nbsp Tianyu Song, &nbsp Wenhao Gu</p>
					</br>	


				</section>

			</div>

			<!-- Footer -->


			<!-- Copyright -->
			<div id="copyright">
					<ul><li>Email: lhuixia1@jhu.edu</li></ul>
					<!-- <ul><li>Phone: (410) 800-7575</li></ul> -->
			</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>